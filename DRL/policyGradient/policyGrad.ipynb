{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from networks import PolicyNetwork, ValueNetwork\n",
    "from torch import nn, optim\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from utils import check_nan, GAE, get_env, get_weights, norm_adv, test_agent, StateNormalizer\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a537afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDS = './vids'\n",
    "WEIGHTS = \"./weights\"\n",
    "os.makedirs(VIDS, exist_ok=True)\n",
    "os.makedirs(WEIGHTS, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06d4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743ab6f",
   "metadata": {},
   "source": [
    "# VPG with GAE and Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31709ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VPG(env:gym.Env, device:torch.device, actor:PolicyNetwork, critic:ValueNetwork, \n",
    "        α:float=1e-4, γ:float=0.99, λ:float=0.95, epochs:int=400, batch_size:float=5_000, \n",
    "        mini_batch_size:int=32, save_freq:int=100) -> None:\n",
    "    \"\"\"Implementation of a modfied version of OpenAI's VPG that uses GAE\n",
    "    and actor-critic framework to train and optimize an agent to learn\n",
    "    a specified environment.\n",
    "\n",
    "    Parameters:\n",
    "        - env (gym.Env) : the environment to simulate.\n",
    "        - device (torch.device) : the device to put tensors on.\n",
    "        - actor (PolicyNetwork) : the neural net that learns a policy.\n",
    "        - critic (ValueNetwork) : the neural net that learns the value \n",
    "                                  function to evaluate a learned policy.\n",
    "        - α (float) : the learning-rate for the Adam optimizer. Default\n",
    "                      is 1e-4.\n",
    "        - γ (float) : the discount factor for GAE. Default is 0.99.\n",
    "        - λ (float) : the bias-variance tradeoff weight for GAE. Default\n",
    "                      is 0.95.\n",
    "        - epochs (int) : the total number of epochs to simulate. Default is \n",
    "                         400.\n",
    "        - batch_size (int) : the total number (state, action, reward, done)\n",
    "                             tuples to collect for estimating the policy\n",
    "                             gradient and value function. Default is 5_000.\n",
    "        - mini_batch_size (int) : the number of (state, action, reward, done)\n",
    "                                  tuples to collect for optimizing the actor\n",
    "                                  and critic weights/parameters. Default is\n",
    "                                  32.\n",
    "        - save_freq (int) : the number of epochs between successive savings\n",
    "                            of the actor and critic networks. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "\n",
    "    actor.baseMLP.register_forward_hook(check_nan)\n",
    "    \n",
    "    recent_weights = get_weights(dir=WEIGHTS, device=device)\n",
    "    if recent_weights is None:\n",
    "        print(\"No recent weights found. Starting from scratch.\")\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        print(\"Loading most recent weights\")\n",
    "        display.clear_output(wait=True)\n",
    "        actor.load_state_dict(recent_weights['actor'])\n",
    "        critic.load_state_dict(recent_weights['critic'])\n",
    "\n",
    "    gae = GAE(gamma=γ, lamb=λ)\n",
    "    state_normalizer = StateNormalizer(state_dim=env.observation_space.shape[0])\n",
    "    π_opt = optim.Adam(actor.parameters(), lr=α)                # Actor/Policy optimizer\n",
    "    V_opt = optim.Adam(critic.parameters(), lr=α)               # Critic/State Value function optimizer\n",
    "    MSE = nn.MSELoss()\n",
    "\n",
    "    pbar = tqdm(iterable=range(1, epochs+1), desc='Epochs', position=0)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        states, actions, rewards, state_vals, dones = [], [], [], [], []\n",
    "        num_states = 0\n",
    "        obs, info = env.reset()                                                             # s_i\n",
    "        state = state_normalizer(obs)\n",
    "        done = False\n",
    "        \n",
    "        # Collect a trajectory of length K batch_size\n",
    "        while num_states < batch_size :         \n",
    "            state_tensor = (torch.tensor(state, dtype=torch.float32)).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action, log_prob = actor.act(state_tensor)                                    # a_i\n",
    "                state_val = (critic(state_tensor)).detach().item()\n",
    "\n",
    "            next_obs, reward, done, trunc, info = env.step(action.detach().cpu().numpy())     #s_{i+1}, r_i\n",
    "\n",
    "            states += [state]\n",
    "            actions += [action.detach().cpu().numpy()]\n",
    "            rewards += [reward]\n",
    "            state_vals += [state_val]\n",
    "            dones += [done]\n",
    "            num_states += 1\n",
    "\n",
    "            if done:\n",
    "                obs, info = env.reset()\n",
    "                state = state_normalizer(obs)\n",
    "                done = False\n",
    "            else:\n",
    "                state = state_normalizer(next_obs) \n",
    "\n",
    "        # Add value of state s_K\n",
    "        if done:\n",
    "            values += [0.0]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                next_state = state_normalizer(next_obs)\n",
    "                last_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "                state_vals += [(critic(last_state_tensor)).detach().item()]\n",
    "\n",
    "        advantages = gae(rewards=rewards, state_vals=state_vals, dones=dones)\n",
    "        returns = [advantage + state_vals[i] for i, advantage in enumerate(advantages)]\n",
    "\n",
    "        states_tensor = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "        actions_tensor = torch.tensor(np.array(actions), dtype=torch.float32).to(device)\n",
    "        returns_tensor = torch.tensor(np.array(returns), dtype=torch.float32).to(device)\n",
    "        advantages_tensor = norm_adv(advantages, return_tensor=True, device=device)\n",
    "        trajectories = TensorDataset(states_tensor, actions_tensor, advantages_tensor, returns_tensor)\n",
    "        trajectory_loader = DataLoader(dataset=trajectories, batch_size=mini_batch_size, shuffle=True)\n",
    "        num_mini_batches = len(trajectory_loader)\n",
    "\n",
    "        # Actor update\n",
    "        pseudolosses = np.zeros(shape=num_mini_batches)\n",
    "        batch_bar = tqdm(iterable=range(1, num_mini_batches+1), desc='Actor Mini Batches', position=0)\n",
    "        for i, (state_batch, action_batch, advantage_batch, _) in enumerate(trajectory_loader):\n",
    "            μs, σs = actor(state_batch)\n",
    "            distrib = Normal(loc=μs, scale=σs)\n",
    "            log_prob_batch = (distrib.log_prob(action_batch)).sum(dim=-1)\n",
    "\n",
    "            pseudoloss = -((log_prob_batch * advantage_batch).mean())\n",
    "            pseudolosses[i] = pseudoloss.detach().cpu().item()\n",
    "\n",
    "            π_opt.zero_grad()\n",
    "            pseudoloss.backward()\n",
    "            π_opt.step()\n",
    "\n",
    "            batch_bar.set_postfix_str(f\"Actor Pseudoloss:{pseudolosses[i]:.5e}\")\n",
    "            batch_bar.update()\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        # Critic update\n",
    "        losses = np.zeros(shape=len(trajectory_loader))\n",
    "        batch_bar = tqdm(iterable=range(1, num_mini_batches+1), desc='Critic Mini Batches', position=0)\n",
    "        for i, (state_batch, _, _, returns_batch) in enumerate(trajectory_loader):\n",
    "            state_val_batch = (critic(state_batch)).squeeze(dim=-1)\n",
    "            loss = MSE(state_val_batch, returns_batch)\n",
    "            losses[i] = loss.detach().cpu().item()\n",
    "\n",
    "            V_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            V_opt.step()\n",
    "\n",
    "            batch_bar.set_postfix_str(f\"Critic MSE Loss:{losses[i]:.5e}\")\n",
    "            batch_bar.update()\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        pbar.set_postfix_str(f\"Averages - Actor Pseudoloss:{pseudolosses.mean():.5e} Critic MSE Loss:{losses.mean():.5e}\")\n",
    "        pbar.update()\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        if epoch % save_freq == 0 or (epoch == epochs):\n",
    "            actor_filename = '/actor_weights_vpg' + f\"_epoch{epoch}.pth\"\n",
    "            critic_filename = '/critic_weights_vpg' + f\"_epoch{epoch}.pth\"\n",
    "            torch.save(actor.state_dict(), WEIGHTS+actor_filename)\n",
    "            torch.save(critic.state_dict(), WEIGHTS+critic_filename)\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb2921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscarescobar/Desktop/BYU/ACME/gymEnv/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/oscarescobar/Desktop/BYU/ACME/LabDev/DRL/policyGrad/vids folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "robot = get_env(\"Humanoid-v5\", vid_dir=VIDS)\n",
    "policy_net = (PolicyNetwork(robot.observation_space.shape[0], robot.action_space.shape[0])).to(device)\n",
    "value_net = (ValueNetwork(robot.observation_space.shape[0], hidden_size=100)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab48a23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 5/5 [00:01<00:00,  2.90it/s, Averages - Actor Pseudoloss:-7.66850e-01 Critic MSE Loss:3.03769e+02]\n",
      "Actor Mini Batches: 100%|██████████| 4/4 [00:00<00:00, 20.59it/s, Actor Pseudoloss:-2.71356e+00] \n",
      "Critic Mini Batches: 100%|██████████| 4/4 [00:00<00:00, 23.76it/s, Critic MSE Loss:3.24883e+02] \n"
     ]
    }
   ],
   "source": [
    "VPG(env=robot, device=device, actor=policy_net, critic=value_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
